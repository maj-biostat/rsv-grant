---
title: "Simulation - RSV study evaluating RSVpreF vs IG"
author: "maj"
date: "2024-11-13"
date-modified: last-modified
bibliography: refs.bib
csl: biomed-central.csl
embed-resources: true
---

```{r}  
#| label: setup
#| code-summary: Libraries and globals
#| code-fold: true

suppressWarnings(suppressPackageStartupMessages(library(cmdstanr)))
library(data.table)
library(ggplot2)
library(parallel)

set.seed(92818785)
# Simulation controls
n_sim <- 10000

# Assume somewhat higher risk on the RSVpreF arm than observed in the MATISSE study
p_0 <- 0.05

p_1 <- p_0 + seq(-0.025, 0.025, len = n_sim)

# Effects based on risk difference and/or VE
rd <- p_1 - p_0
ve <- 1-(p_1/p_0)


N_per_arm <- 1500

mc_cores <- 60
```

## Background

### Aim

We 


### Interventions


### Randomisation


### Primary outcome

Options:

+ binary outcome variable indicating medically attended per the definition of MATISSE study:

  + MA-RTI visit AND RSV-positive test result AND one or more of the following
    + Fast breathing (RR $\ge 60$  bpm for $<2$  months of age [$<60$ days of age], $\ge 50$ bpm for $2 - <12$ months of age, or $\ge 40$ bpm for 12â€“24 months of age)
    + SpO2 $<95%$
    + Chest wall indrawing

+ ordinal variable indicating health state at each follow up:

  + death
  + medical attendance for respiratory tract infection
  + medical attendance for respiratory tract infection + RSV positive test
  + hospitalised for respiratory tract infection + RSV positive test
  + hospitalised for severe respiratory tract infection + RSV positive test

### Estimand

Primary: population perspective, i.e. the estimated comparative effectiveness of each implemented strategy with adherence as observed under real-world conditions.

### Follow up

Follow up to age 360 days for both groups.

### Prior information

MATISSE study (run during COVID-19) is the primary source of information for day 360 outcomes for RSVpreF (maternal vaccine).
For the medically attended lower RTI with RSV positive test (RSV-MA-LRTI, see Tables S1 and S6 in MATISSE supplementary authored by Kampmann, i.e. interim analysis), there were $92/3495 \approx 2.6%$ cases on RSVpreF arm and $156/3480 \approx 4.5%$ cases on the placebo arm, $VE \approx 0.41$ at day 360.

No day 360 data available for immunoprophylaxis (nirsevimab).

### Constraints

Sample size up to approximately 3000 dyads.

## Methods

A high-level description of the analysis options include:

+ Conjugate beta binomial analysis comparing the number of binary events by arm at a cross sectional timepoint (e.g. 360 days)
+ Leverage historical controls via partial pooling
+ Logistic regression comparing the number of binary events by arm adjusting for prognostic covariates
+ Longitudinal analysis 


## Operating characteristics

### Reference design

As a reference design, assume the least informative binary indicator with single analysis at maximum sample size (3000 dyads).

Analysis based on conjugate beta binomial model.

Consider two possible trial conclusion criteria, tailored dependening on what is more relevant.
These are arbitrary and applied indepdently, just to motivate discussion.

1. Conclude that IG is more effective if there is a 97.5% (or higher) probability that it reduces the risk of MA-RTI relative to the RSVpreF arm. That is, $Pr(p_1 < p_0) > 0.975 \implies \text{trial success}$. The control arm (RSVpreF) is assumed to have an event risk of $p_0 = 0.05$ and the IG group event risk $p_1 \in \{x : 0.025 \le x \le 0.075\}$ where values lower than $p_0$ imply benefit associated with IG relative to RSVpreF.
2. Conclude that IG is more effective if there is a 90% (or higher) probability that the VE, defined as 1 - RR, is above 20%. That is, $Pr(1 - (p_1 / p_0) > 0.2) > 0.9 \implies \text{trial success}$. VE is computed from the same values nominated as above. When evaluating two active vaccines, it is possible for VE to be less than zero, e.g. when the interventional arm increases risk, $p_1 > p_0$. This approach is simply stating that there needs to be some minimal non-zero effect present, but does not require such a stringent decision criteria.

The power curve is based on interpolating the results from `r n_sim` simulated trials.

```{r}
#| label: oc-1
#| code-summary: Operating characteristics for least efficient design
#| code-fold: true
    
    
y_0 <- rbinom(n_sim, N_per_arm, p_0)
y_1 <- rbinom(n_sim, N_per_arm, p_1)

i <- 1

# Define the function to integrate
integrand_1 <- function(
    x, alpha_0, beta_0,
    alpha_1, beta_1) {
  pbeta(x, alpha_0, beta_0) * dbeta(x, alpha_1, beta_1)
}
  
# Say we are interested in VE  = 1 - (p1/p0) and specifically in the probability
# that Pr(VE > 0.2) = Pr(1 - p1/p0 > 0.2) = Pr(p1/p0 < 0.8).

# We need to integrate over all possible values of p_0 and p_1 considering only the
# cases where p_1/p_0 < 0.8. So we need a double integral
# Pr(p1/p0 < 0.8) = \int_0^1 \int_0^{0.8p_0} f_1(p_1) f_0(p_0) dp_1 dp_0
# This integral accumulates the probability that, given fixed p_0, p_1 falls below 
# 0.8 * p_0
integrand_2 <- function(
    p0, alpha_0, beta_0,
    alpha_1, beta_1, delta = 0) {
  sapply(p0, function(p0_val) {
    integrate(function(p1) dbeta(p1, alpha_1, beta_1), 
              lower = 0, 
              upper = delta * p0_val)$value * dbeta(p0_val, alpha_0, beta_0)
  })
}

res <- mclapply(1:n_sim, function(i){
  
  # i = sample(1:n_sim, 1)
  # Evaluate whether risk of event is lower on IG arm
  pr_1_lt_0 <- integrate(
    integrand_1, lower = 0, upper = 1,
    alpha_0 = y_1[i] + 1 , 
    beta_0 = N_per_arm - y_1[i] + 1 ,
    alpha_1 = y_0[i] + 1, 
    beta_1 = N_per_arm - y_0[i] + 1)$value
  
  pr_ve_gt_delta <- integrate(
    integrand_2, lower = 0, upper = 1,
    alpha_0 = y_0[i] + 1  , 
    beta_0 = N_per_arm - y_0[i] + 1 ,
    alpha_1 = y_1[i] + 1, 
    beta_1 = N_per_arm - y_1[i] + 1,
    delta = 1 - 0.2)$value
  
  # p_post_0 <- rbeta(1e5, y_0[i] + 1, N_per_arm - y_0[i] + 1)
  # p_post_1 <- rbeta(1e5, y_1[i] + 1, N_per_arm - y_1[i] + 1)
  # c(pr_1_lt_0_int, mean(p_post_1 < p_post_0))
  # ve <- 1 - (p_post_1/p_post_0)
  # c(pr_ve_gt_delta, mean(ve > 0.2))
  
  # # equivalent to testing risk difference against null hypothesis of zero
  # pr_1_lt_0_approx <- mean(p_post_1 < p_post_0)
  
  # assume a typical frequentist critria
  c(pr_1_lt_0 = pr_1_lt_0, 
    pr_ve_gt_delta = pr_ve_gt_delta)
  
  
}, mc.cores = mc_cores)

m_res <- do.call(rbind, res)

d_fig_1 <- data.table(
  x_rd = rd,
  x_ve = ve,
  win_rd = as.numeric(m_res[, 1] > 0.975),
  win_ve = as.numeric(m_res[, 2] > 0.9)
)
```

```{r}
#| echo: FALSE
#| label: fig-pwr_1_rd
#| fig-cap: 'Power curves characterising power by effect size (risk difference) at a sample size of 1500 per arm in two group study with binary outcome'
#| fig.height: 4.5
#| fig.width: 4.5
#| fig-pos: H


ggplot(d_fig_1, aes(x = x_rd, y = win_rd)) +
  geom_smooth(se = F, lwd = 0.3, col = 1, 
              method = "gam", formula = y ~ s(x, bs = "ps")) +
  scale_y_continuous("Power", breaks = seq(0, 1, by = 0.1)) +
  scale_x_continuous("Risk difference (p1-p0)", breaks = seq(-0.02, 0.05, by = 0.01)) +
  theme_minimal()
```


```{r}
#| echo: FALSE
#| label: fig-pwr_1_ve
#| fig-cap: 'Power curves characterising power by effect size (vaccine effectiveness) at a sample size of 1500 per arm in two group study with binary outcome'
#| fig.height: 4.5
#| fig.width: 4.5
#| fig-pos: H


ggplot(d_fig_1, aes(x = x_ve, y = win_ve)) +
  geom_smooth(se = F, lwd = 0.3, col = 1, 
              method = "gam", formula = y ~ s(x, bs = "ps")) +
  scale_y_continuous("Power", breaks = seq(0, 1, by = 0.1)) +
  scale_x_continuous("Vaccine effectiveness (1 - RR)", breaks = seq(-1, 0.4, by = 0.2)) +
  theme_minimal()
```



### Historic controls


```{r}
#| echo: false
#| code-fold: true

m1 <- cmdstanr::cmdstan_model("../stan/historic-control-1.stan")
```

```{r}
#| label: setup2
#| code-summary: Configuring inputs for simulation
#| code-fold: true
    
n_sim <- 2500

# Only look at subset of original p_1
p_1_sub <- quantile(p_1, probs = c(0, 0.25, 0.5, 0.75, 1))

# Prior for treatment group (IG)
# Convert beta parameterisation from mean and sample size to a and b
alpha <- function(mu, nu){
  mu * nu
} 
beta <- function(mu, nu){
  (1-mu)*nu
}

# set mean to 
mu <- 0.05
nu <- 10.05

j = 1

# Effects based on risk difference and/or VE
rd_sub <- p_1_sub - p_0
ve_sub <- 1-(p_1_sub/p_0)

N_ctl <- 1500
N_trt <- 1500
```

Incorporating historic controls can be down within a power-prior framework.
There are lots of ways to implement power-priors, @Ibrahim2015.
Here, the partial pooling approach is used:

$$
\begin{aligned}
Y_0 &\sim \text{Bin}(n_0, p_0) \quad \text{Current control arm RSVpreF} \\
Y_1 &\sim \text{Bin}(n_1, p_1) \quad \text{Current treatment arm IG} \\

Y_h &\sim \text{Bin}(n_h, p_h) \quad \text{Historic data for RSVpreF from trial } h \\
\text{logit}(p_0), \text{logit}(p_h) &\sim \text{N}(\mu, \tau) \quad \text{Common distribution for controls}\\
\mu &\sim \text{Normal}(\text{logit}(0.05), 0.25) \\
\tau &\sim \text{Exp}(3) \\
p_1 &\sim \text{Beta}(0.5, 9.5)
\end{aligned}
$$

meaning $\mu$ and $\tau$ represent the between study mean and SD. 
When $\tau$ is small, all the logits are similar, so it is appropriate to leverage historic information.
When $\tau$ is large, we have observed high variability in the control rate and thus we do not want to incorporate much of the earlier data.
Quantities such as risk difference and VE are then derived as functions of the above.

With only a single trial, we have negligible data to inform the distribution of results on RSVpreF at 360 days.
Therefore, the results are highly dependent on whether the current trial aligns with the historic trial along with the assumptions we make about the trial to trial variability of risk on the RSVpreF arm.

The estimated risk on the RSVpreF arm in both the historic and current trial will be pulled towards one another.
If the historic data for the RSVpreF arm is associated with a lower risk, then the current data will be pulled towards this lower value and vice versa.
In the former case, power will usually be impeded, in the latter case, power will usually increase.

For example, think about a specific case where we look at the risk difference.
Assume that the true $p_0 = 0.05$ but the historic control says $p_h = 0.02$. 
At the same time assume that $p_1 = 0.03$.
With no pooling $rd = p_1 - p_0 = 0.03 - 0.05 = -0.02$ but when we partially pool the control arm, we will end up with something more like $rd = p_1 - p_0 = 0.03 - 0.04 = -0.01$ because the $p_0$ arm has been pulled towards $p_h = 0.02$.
Demonstration below (simulated trials `r n_sim` for scenarios with $p_0$ set as before and $p_1$ set to `r paste0(p_1_sub, collapse = ", ")`.
An informative prior was put on the risk under IG that aligns with a belief that there is a 95% chance that the risk of the outcome is lower than 20%.

```{r}
#| label: historic-ctl-sim-1
#| code-summary: Simulation implementation based on single historic control 
#| code-fold: true

# only supports a single historic control for the moment because that
# is all we have.
sim_historic_control <- function(yh = 92, nh = 3495){
  
  # to store results
  d_res <- data.table(
    rd = rd_sub, ve = ve_sub,
    pwr_rd = NA_real_, pwr_ve = NA_real_
  )
  
  for(j in seq_along(p_1_sub)){
    
    y_0 <- rbinom(n_sim, N_ctl, p_0)
    y_1 <- rbinom(n_sim, N_trt, p_1_sub[j])
    
    res <- mclapply(1:n_sim, function(i){
      
      ld <- list(
        n0 = N_ctl, y0 = y_0[i], 
        nt = N_trt, yt = y_1[i],
        
        H = 1,
        yh = yh, 
        nh = nh,
        
        # centre at 5% with 2.5 to 10% being well within realms of possibility
        # on the risk scale, converted to log odds scale
        pri_mu = c(qlogis(0.05), 0.25),
        pri_tau_rho = 3, 
      
        # Set the mean to 6% and the pre-existing sample size to 10
        # This is consistent with a 95% probability that the risk on IG at 360 days
        # is less than 20% and a 99% probability that the risk on IG at 360 is less
        # that 32%
        pri_pt = c(alpha(mu, nu), beta(mu, nu))
      )
    
      f1 <- m1$sample(
        ld, iter_warmup = 1000, iter_sampling = 2000,
        parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = T,
        max_treedepth = 10, adapt_delta = 0.99)
      
      # snk <- capture.output(
      #   f1 <- m1$pathfinder(ld, num_paths=20, single_path_draws=200,
      #                       history_size=50, max_lbfgs_iters=100,
      #                       refresh = 0, draws = 2000)
      #   )
    
      d_tmp <- data.table(f1$summary(
        variables = c("rd"),
        prob_dec = ~ mean(. < 0)))
      
      win_rd <- d_tmp[variable == "rd", prob_dec > 0.975]
      
      d_tmp <- data.table(f1$summary(
        variables = c("ve"),
        prob_dec = ~ mean(. > 0.2)))
      
      win_ve <- d_tmp[variable == "ve", prob_dec > 0.9]
      
      c(win_rd = win_rd, win_ve = win_ve)
      
    }, mc.cores = mc_cores)
    
    pwr <- colMeans(data.table(do.call(rbind, res)))
    d_res[j, pwr_rd := pwr[1]]
    d_res[j, pwr_ve := pwr[2]]
    
  }
  d_res
}


d_fig_2a <- copy(sim_historic_control())
d_fig_2b <- copy(sim_historic_control(yh = 175, nh = 3495))

d_fig_2 <- rbind(
  cbind(data.table(p_h = sprintf("%.1f%%", 100*92/3495)), d_fig_2a),
  cbind(data.table(p_h = sprintf("%.1f%%", 100*175/3495)), d_fig_2b)
)

```



```{r}
#| echo: FALSE
#| label: fig-pwr_2_rd
#| fig-cap: 'Power curves characterising power by effect size (risk difference) showing impact when historic control is lower and when the historic control is aligned with the present data.'
#| fig.height: 4.5
#| fig.width: 4.5
#| fig-pos: H

tit <- sprintf("Power curve (N_trt = %.0f, N_ctl = %.0f)", N_trt, N_ctl)
ggplot(d_fig_1, aes(x = x_rd, y = win_rd)) +
  geom_smooth(se = F, lwd = 0.3, col = 1, 
              method = "gam", formula = y ~ s(x, bs = "ps")) +
  geom_point(data = d_fig_2,
             aes(x = rd, y = pwr_rd, col = p_h),
             inherit.aes = F) +
  scale_y_continuous("Power", breaks = seq(0, 1, by = 0.1)) +
  scale_x_continuous("Risk difference (p1-p0)", breaks = seq(-0.02, 0.05, by = 0.01)) +
  scale_color_discrete("Risk in historic study for RSVpreF") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  ggtitle(tit)
```




### Alternative approaches

```{r}
#| code-summary: Models for ordinal analysis
#| echo: false
#| code-fold: true

m2 <- cmdstanr::cmdstan_model("./stan/ordinal-01.stan")
m3 <- cmdstanr::cmdstan_model("./stan/ordinal-02.stan")
m4 <- cmdstanr::cmdstan_model("./stan/logistic.stan")
```

```{r}
#| label: ordinal-setup
#| code-summary: Setup constants for ordinal outcome variable 
#| code-fold: true


# Define categories and probabilities under standard treatment
cats <- c(
  "death", #  - all cause
  "icu", #  - all cause
  "hosp rsv+",  
  "MA rsv+",  
  "MA rsv-", 
  "MA rti", 
  "No MA")

p_soc <- c(0.01, 0.01, 0.03, 0.05, 0.05, 0.55, 0.3) 
names(p_soc) <- cats
stopifnot(sum(p_soc) == 1)
p_soc


compute_probs <- function(p_soc, or_new_trt){
  cum_p_soc <- cumsum(p_soc)
  # Logit transformation of cumulative probabilities for standard treatment
  lo_cum_probs_soc <- qlogis(cum_p_soc[-length(cum_p_soc)])
  # Adjust logit cumulative probabilities for new treatment
  lo_cum_p_new <- lo_cum_probs_soc + log(or_new_trt)
  cum_p_new <- plogis(lo_cum_p_new)
  cum_p_new <- c(cum_p_new, 1)  # Ensure last category is 1
  p_new <- diff(c(0, cum_p_new))  # Convert
  m <- rbind(p_new, p_soc)  
  colnames(m) <- cats
  m
}

compute_probs(p_soc, or_new_trt = 0.7)

```


If a set of applicable outcomes could be ranked, then an ordinal outcome might lead to power gains.

Under a cross-sectional perspective, a units' worst state over the 360 days since birth is taken as the observation for that outcome, which is effectively just providing some finer granularity to what was previously an indicator variable. 
The approach might evaluate treatment effects directly or use the model to estimate probabilities of being at various states. 
Clearly, we are making additional assumptions here, but we are also leveraging additional information without having to rely on historic data.

```{r}
#| label: ordinal-data
#| code-summary: Data generation for ordinal outcome definition 
#| code-fold: true


# only supports a single historic control for the moment because that
# is all we have.
get_ordinal_data <- function(
    N = 3000, 
    or_new_trt = 0.7,
    # death, 
    # hosp rti rsv+, 
    # hosp rti, 
    # ma rti rsv+, 
    # ma rti, 
    # no ma for rti  
    p_soc = c(0.01, 0.01, 0.03, 0.05, 0.05, 0.55, 0.3) 
    ){
  
  cum_p_soc <- cumsum(p_soc)  
  # Logit transformation of cumulative probabilities for standard treatment
  lo_cum_probs_soc <- qlogis(cum_p_soc[-length(cum_p_soc)])


  # Adjust logit cumulative probabilities for new treatment
  lo_cum_p_new <- lo_cum_probs_soc + log(or_new_trt)
  cum_p_new <- plogis(lo_cum_p_new)
  cum_p_new <- c(cum_p_new, 1)  # Ensure last category is 1
  p_new <- diff(c(0, cum_p_new))  # Convert back to category probabilities

  # 1:1 allocation
  trt <- rep(c("SoC", "New"), each = N / 2)
  p_mat <- rbind(p_soc, p_new)
    
  outcomes <- unlist(lapply(1:length(trt), function(i) {
    sample(cats, 1, prob = p_mat[(trt[i] == "New") + 1, ])
  }))
    
  data.table(trt, outcome = factor(outcomes, levels = rev(cats)))

}

# N <- 3000
# d_trial <- get_ordinal_data(N, or_new_trt)
# d_smry <- d_trial[, .(p_obs = .N/(N/2)), keyby = .(trt, outcome)]
# d_smry <- cbind(d_smry, p_tru = c(rev(p_new), rev(p_soc)))
# d_smry
```

Comparison of binary vs ordinal perspective on single data set.

```{r}
d_trial <- get_ordinal_data(N = 3000, 0.7)
d_trial[outcome == "MA rsv+", y_bin := 1]
d_trial[outcome != "MA rsv+", y_bin := 0]
d_trial[trt == "SoC", x := 0]
d_trial[trt == "New", x := 1]

h1 <- glm(y_bin ~ x, data = d_trial, family = "binomial")

d_trial[, outcome := factor(outcome, levels = cats)]
h2 <- MASS::polr(outcome ~ x, data = d_trial, Hess = TRUE)

# trt arm
marginaleffects::predictions(h1, newdata = data.table(x = 0:1), type = "response")
# compare results to those for MA rsv+ below
marginaleffects::predictions(h2, newdata = data.table(x = 0:1), type = "probs")
```


```{r}
#| label: ordinal-sim-1
#| code-summary: Simulation implementation based on ordinal outcome definition 
#| code-fold: true

risk_diff_on_po_opt <- function(or_new_trt, rd_target){
  cum_p_soc <- cumsum(p_soc)  
  # Logit transformation of cumulative probabilities for standard treatment
  lo_cum_probs_soc <- qlogis(cum_p_soc[-length(cum_p_soc)])


  # Adjust logit cumulative probabilities for new treatment
  lo_cum_p_new <- lo_cum_probs_soc + log(or_new_trt)
  cum_p_new <- plogis(lo_cum_p_new)
  cum_p_new <- c(cum_p_new, 1)  # Ensure last category is 1
  p_new <- diff(c(0, cum_p_new))  # Convert back to category probabilities
  
  (rd_target - (p_new[4] - p_soc[4]))^2
}

# Compute the ORs that are consistent with risk differences that we 
# have looked at previously.
p_1_sub <- c(0.0250, 0.0375, 0.0425, 0.0500, 0.0625)
p_1_sub - p_0
or_new_trt <- length(p_1_sub)
for(i in seq_along(p_1_sub)){
  
  or_new_trt[i] <- optimize(
    risk_diff_on_po_opt, 
    c(0, 10), tol = 0.0001, 
    rd_target = p_1_sub[i] - p_0)$minimum
  
}

rd_sub <- p_1_sub - p_0

sim_ordinal <- function(){
  
  
  d_grid <- CJ(trt = c("SoC", "New"), outcome = cats)
  d_grid[, outcome := factor(outcome, levels = cats)]
  
  l_res <- list()
  
  i <- j <- 3
  for(j in seq_along(or_new_trt)){
    
    l_res[[j]] <- mclapply(1:n_sim, function(i){
      
      d_trial <- get_ordinal_data(N = 3000, or_new_trt[j])
      
      d_trial[trt == "New", x := 1]
      d_trial[trt == "SoC", x := 0]
      d_trial <- d_trial[order(x)]
      
      
      d_trial_grp <- d_trial[, .N, keyby = .(trt, x, outcome)]
      # Accounting for chance that some groups have zero counts
      d_trial_grp <- merge(d_trial_grp, d_grid, by = c("trt", "outcome"), all.y = T)
      d_trial_grp[is.na(x) & trt == "New", x := 1]
      d_trial_grp[is.na(x) & trt == "SoC", x := 0]
      d_trial_grp[is.na(N), N := 0]
      
      d_trial_grp[, p_obs := N / (nrow(d_trial)/2)]
      d_trial_grp <- d_trial_grp[order(x)]
      
      d_trial_bin <- copy(d_trial_grp)
      d_trial_bin[outcome == "MA rsv+", evt := 1]
      d_trial_bin[outcome != "MA rsv+", evt := 0]
      d_trial_bin <- d_trial_bin[, .(N = sum(N)), keyby = .(trt, evt)]
      d_trial_bin <- dcast(d_trial_bin, trt ~ evt, value.var = "N")
      setnames(d_trial_bin, "0", "failure")
      setnames(d_trial_bin, "1", "success")
      d_trial_bin[, n := failure + success]
      d_trial_bin[trt == "New", x := 1]
      d_trial_bin[trt == "SoC", x := 0]
      d_trial_bin <- d_trial_bin[order(x)]
      d_trial_bin[, p_obs := success/n]
      
      ld <- list(
        N = nrow(d_trial_grp),
        K = length(cats),
        P = 1,
        y = as.integer(d_trial_grp$outcome),
        X = as.matrix(d_trial_grp$x, ncol = 1, drop = F ),
        wgt = d_trial_grp$N,
        pri_b_s = 3, pri_cuts_s = 3
      )
      
      f4 <- m3$sample(
        ld, iter_warmup = 1000, iter_sampling = 2000,
        parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = F,
        max_treedepth = 10, adapt_delta = 0.99)
      
      
      ld <- list(
        N = nrow(d_trial_bin),
        y = d_trial_bin$success, n = d_trial_bin$n,
        X = as.matrix(d_trial_bin$x, ncol = 1, drop = F )
      )
      
      f5 <- m4$sample(
        ld, iter_warmup = 1000, iter_sampling = 2000,
        parallel_chains = 1, chains = 1, refresh = 0, show_exceptions = T,
        max_treedepth = 10, adapt_delta = 0.99)
      
      d_tmp <- data.table(f4$draws(variables = c("p0", "p1"), format = "matrix"))
      d_post <- melt(d_tmp, measure.vars = names(d_tmp))
      d_post[, ix := gsub("p.\\[", "", variable, fixed = F)]
      d_post[, ix := as.numeric(gsub("\\]", "", ix, fixed = F))]
      d_post[variable %like% "p0", trt := "SoC"]
      d_post[variable %like% "p1", trt := "New"]
      d_smry_ord <- d_post[, .(p_mu_ord = mean(value)), keyby = .(trt, ix)]
      d_smry_ord[, outcome := cats[length(cats) + 1 -ix]]
      d_smry_ord[, ix := NULL]
      d_smry_ord <- merge(d_smry_ord, d_trial_grp, by = c("trt", "outcome"), all = T)
      
      d_tmp <- data.table(f5$draws(variables = c("p0", "p1"), format = "matrix"))
      d_post <- melt(d_tmp, measure.vars = names(d_tmp))
      d_post[variable %like% "p0", trt := "SoC"]
      d_post[variable %like% "p1", trt := "New"]
      d_smry_bin <- d_post[, .(p_mu_bin = mean(value)), keyby = .(trt)]
      d_smry_bin[, outcome := "MA rsv+"]
      
      d_smry_p <- merge(d_smry_ord, d_smry_bin, by = c("trt", "outcome"), all.x = T)
      setcolorder(d_smry_p, c("trt", "x", "outcome", "N", "p_obs", "p_mu_ord", "p_mu_bin"))
      
      d_smry_p[, outcome := factor(outcome, levels = cats)]
      d_smry_p <- d_smry_p[order(outcome, x)]
      
      
      d_tmp <- data.table(f4$draws(variables = c("rd"), format = "matrix"))
      d_post <- melt(d_tmp, measure.vars = names(d_tmp))
      d_post[, ix := gsub("rd\\[", "", variable, fixed = F)]
      d_post[, ix := as.numeric(gsub("\\]", "", ix, fixed = F))]
      d_smry_ord <- d_post[, .(
        rd_mu_ord = mean(value),
        pr_lt_0 = mean(value < 0)
        ), keyby = .(ix)]
      d_smry_ord[, outcome := cats[length(cats) + 1 -ix]]
      d_smry_ord[, ix := NULL]
      d_smry_ord <- merge(
        d_smry_ord, 
        dcast(d_trial_grp, outcome ~ trt, value.var = "p_obs")[, .(outcome, rd_obs = New - SoC)], 
        by = c("outcome"), all = T)
      
      d_tmp <- data.table(f5$draws(variables = c("rd"), format = "matrix"))
      d_post <- melt(d_tmp, measure.vars = names(d_tmp))
      d_smry_bin <- d_post[, .(rd_mu_bin = mean(value))]
      d_smry_bin[, outcome := "MA rsv+"]
      
      d_smry_rd <- merge(d_smry_ord, d_smry_bin, by = c("outcome"), all.x = T)
      d_smry_rd[outcome == "MA rsv+", rd_sim := rd_sub[j]]
      
      setcolorder(d_smry_rd, c("outcome", "rd_sim", "rd_obs", "rd_mu_ord", "rd_mu_bin"))
      d_smry_rd[, outcome := factor(outcome, levels = cats)]
      d_smry_rd <- d_smry_rd[order(outcome)]
      
      
      d_tmp <- data.table(f4$draws(variables = c("b"), format = "matrix"))
      d_post <- melt(d_tmp, measure.vars = names(d_tmp))
      d_smry_ord_b <- d_post[, .(b_mu_bin = mean(value),
                             q_025 = quantile(value, prob = 0.025),
                             q_975 = quantile(value, prob = 0.975),
                             pr_lt_0 = mean(value < 0))]
      d_smry_ord_b[, b_sim := round(log(or_new_trt[j]), 5)]
      
            
      d_tmp <- data.table(f5$draws(variables = c("b"), format = "matrix"))
      d_post <- melt(d_tmp, measure.vars = names(d_tmp))
      d_smry_bin_b <- d_post[, .(b_mu_bin = mean(value),
                             q_025 = quantile(value, prob = 0.025),
                             q_975 = quantile(value, prob = 0.975),
                             pr_lt_0 = mean(value < 0))]
      
      
      d_smry_b <- rbind(
        cbind(model = "ordinal", d_smry_ord_b),
        cbind(model = "logistic", d_smry_bin_b),
        fill = T
      )
      setcolorder(d_smry_b, c("model", "b_sim"))
      
      list(
        d_smry_p = d_smry_p,
        d_smry_rd = d_smry_rd,
        d_smry_b = d_smry_b
      )
      
    }, mc.cores = mc_cores)
    
  }
  
  l_res
}



l_res_sim <- sim_ordinal()



```


```{r}
#| label: ordinal-sim-processing
#| code-summary: Simulation implementation based on ordinal outcome definition 
#| code-fold: true

d_smry_p <- rbindlist(lapply(1:length(l_res_sim), function(ii){
  d_smry_p <- rbindlist(lapply(1:length(l_res_sim[[jj]]), function(jj){
    l_res_sim[[ii]][[jj]]$d_smry_p  
  }), idcol = "i_sim")  
  d_smry_p
}), idcol = "i_scenario")

d_smry_rd <- rbindlist(lapply(1:length(l_res_sim), function(ii){
  d_smry_rd <- rbindlist(lapply(1:length(l_res_sim[[jj]]), function(jj){
    l_res_sim[[ii]][[jj]]$d_smry_rd  
  }), idcol = "i_sim")  
  d_smry_rd
}), idcol = "i_scenario")

d_smry_b <- rbindlist(lapply(1:length(l_res_sim), function(ii){
  d_smry_b <- rbindlist(lapply(1:length(l_res_sim[[jj]]), function(jj){
    l_res_sim[[ii]][[jj]]$d_smry_b  
  }), idcol = "i_sim")  
  d_smry_b
}), idcol = "i_scenario")


d_smry_b[, .(pwr = mean(pr_lt_0 > 0.975)), keyby = .(i_scenario, model)]
```


<!-- ::: {#refs} -->
<!-- ::: -->





